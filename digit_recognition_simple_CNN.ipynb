{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is another version of [this](https://github.com/vibvibgyor/keras_tutorials/blob/master/digit_recognition_simple_NN.ipynb) tutorial, where simple Neural network is used for training. In this tutorial [Convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) is be used. The aim is to train a convolutional neural network that will detect digits from 0-9. For training the convolutional neural network, MNIST (Modified National Institute of Standards and Technology database) dataset is used. The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
    "\n",
    "MNIST dataset is provided with the keras so, you can directly use it without downloading it separately.\n",
    "\n",
    "Also note that this is a single label, multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/vibhanshu/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/vibhanshu/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import the dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# load the data using load_data()\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is encoded as Numpy arrays. Let's look at the shape and length of these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data:  (60000, 28, 28)\n",
      "The shape of the training labels:  (60000,)\n",
      "The shape of the testing data:  (10000, 28, 28)\n",
      "The shape of the testing labels:  (10000,)\n",
      "The length of the training data:  60000\n",
      "The length of the training labels:  60000\n",
      "The length of the testing data:  10000\n",
      "The length of the testing labels:  10000\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the training data: ', train_images.shape)\n",
    "print('The shape of the training labels: ', train_labels.shape)\n",
    "print('The shape of the testing data: ', test_images.shape)\n",
    "print('The shape of the testing labels: ', test_labels.shape)\n",
    "print('The length of the training data: ', len(train_images))\n",
    "print('The length of the training labels: ', len(train_labels))\n",
    "print('The length of the testing data: ', len(test_images))\n",
    "print('The length of the testing labels: ', len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 6000 images of size 28 x 28 for training and 10000 images of size 28 x 28 for testing. The shape of the train and test labels is (6000,) which means that it is a list of labels which are mapped to each image in the train and test set respectively. \n",
    "\n",
    "Now, let's look at the contents of label arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that these labels are numbers from 0-9. You can also see the images using matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADkhJREFUeJzt3X+MVfWZx/HPo7QxQo0oMzJQ2GkJWRXj0vVKNnGysmkksJJAE2uYRKSRMP0DjSWNWYNK+UMN0a1d4o+aYZkUtdA2KS4EyQKBTbCJIY4GkRYXkMxSlgkzaBMlGgny7B9zcKc493sv99e5zPN+Jebee55z7nk84TPn3vs9937N3QUgnivybgBAPgg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgxjRyZxMmTPD29vZG7hIIpa+vT6dPn7Zy1q0q/GY2V9JaSVdK+nd3X5Nav729Xb29vdXsEkBCoVAoe92KX/ab2ZWSXpQ0T9LNkjrN7OZKnw9AY1Xznn+WpKPufszdz0r6jaQFtWkLQL1VE/7Jkv487PGJbNlfMbMuM+s1s97BwcEqdgeglqoJ/0gfKnzt+8Hu3u3uBXcvtLS0VLE7ALVUTfhPSJoy7PG3JZ2srh0AjVJN+N+WNN3MvmNm35S0SNLW2rQFoN4qHupz93Nm9qCkHRoa6utx9z/WrDMAdVXVOL+7b5e0vUa9AGggLu8FgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqKpm6TWzPkmfSvpS0jl3L9SiKQD1V1X4M//k7qdr8DwAGoiX/UBQ1YbfJe00s3fMrKsWDQFojGpf9t/h7ifNrFXSLjP7wN33Dl8h+6PQJUlTp06tcncAaqWqM7+7n8xuByS9LmnWCOt0u3vB3QstLS3V7A5ADVUcfjMba2bfunBf0hxJB2vVGID6quZl/w2SXjezC8+z0d3/syZdAai7isPv7sck/V0Ne8EodPjw4aK1zz77rKrnnjRpUrLe2tpa1fOPdgz1AUERfiAowg8ERfiBoAg/EBThB4Kqxbf6cBnbu3dvsv7hhx8m693d3cn6wYPFr/s6c+ZMcttSZsyYkazv2LGjaG3y5MlV7Xs04MwPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzj/K7d69O1l/6aWXkvXNmzdXtf/UT7e1tbVV9dwnT55M1qdNm1a0tn///uS2N954Y7I+ODiYrK9YsSJZP3XqVNHarl27ktvWCmd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiKcf5RYNOmTUVrq1atSm579OjRZL2npydZLzUF2+233160ds011yS3LeW1115L1h955JGitVLXLyxdujRZnz9/frJ+7NixZH3Lli3JeiNw5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoEqO85tZj6T5kgbc/ZZs2XWSfiupXVKfpHvd/S/1azO2/v7+ZH316tVFa6nvjUvSq6++mqwvWrQoWR8zJr9LRUr1tm/fvqK1xx9/PLntM888k6xfddVVyfratWuT9Y6OjmS9Eco58/9K0tyLlj0qabe7T5e0O3sM4DJSMvzuvlfSxxctXiBpQ3Z/g6SFNe4LQJ1V+p7/Bnfvl6TstrV2LQFohLp/4GdmXWbWa2a9pX73DEDjVBr+U2bWJknZ7UCxFd29290L7l5oaWmpcHcAaq3S8G+VtCS7v0RS/l9RAnBJSobfzDZJekvS35rZCTNbKmmNpLvM7Iiku7LHAC4jJQdp3b2zSOn7Ne4FRWzbti1ZP3z4cNFaqXH8++67r6KemsG6deuS9RdeeKHi577zzjuT9Y0bNybrY8eOrXjfjcIVfkBQhB8IivADQRF+ICjCDwRF+IGg+Onuy8CePXuS9dSw0m233Vbrdi7JF198UbRWairqp556Kln/4IMPkvVrr722aK3UV27vueeeZP3qq69O1i8HnPmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjG+S8DpX66+7HHHitau+mmm6ra9/nz55P1N998M1l/9tlni9beeOON5LatremfhlyxYkWyXmp68ug48wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIzzXwauuCL9Nzo1Xr58+fLktuPGjUvWN2zYkKw/8MADyXqq94ceeii57f3335+sFwqFZB1pnPmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKiS4/xm1iNpvqQBd78lW7Za0jJJg9lqK919e72ajK6joyNZT43Fv/XWW8ltn3/++WS9t7c3WZ83b16yvnLlyqK1Uv9fqK9yzvy/kjR3hOW/cPeZ2X8EH7jMlAy/u++V9HEDegHQQNW853/QzA6YWY+Zja9ZRwAaotLw/1LSNEkzJfVL+nmxFc2sy8x6zax3cHCw2GoAGqyi8Lv7KXf/0t3PS1onaVZi3W53L7h7oaWlpdI+AdRYReE3s7ZhD38g6WBt2gHQKOUM9W2SNFvSBDM7Ielnkmab2UxJLqlP0o/r2COAOigZfnfvHGHx+jr0ggodP368aG3u3JFGaf/fxIkTk/WdO3cm67feemuyjubFFX5AUIQfCIrwA0ERfiAowg8ERfiBoPjp7gY4e/Zssr5jx45k/bnnnqt434sXL07We3p6kvUxY/gnMlpx5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjEbYCXX345WX/44YeT9enTpyfrR44cKVor9ZVbxvHj4swPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ExyFsDTzzxRLL+5JNPJuvLli1L1letWpWsz5kzp2ht6tSpyW0RF2d+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq5Di/mU2R9IqkiZLOS+p297Vmdp2k30pql9Qn6V53/0v9Ws3Xnj17ita2bt2a3LarqytZf/rppyvq6YKPPvqoaG3SpElVPTdGr3LO/Ock/dTdb5L0D5KWm9nNkh6VtNvdp0vanT0GcJkoGX5373f3d7P7n0o6JGmypAWSNmSrbZC0sF5NAqi9S3rPb2btkr4naZ+kG9y9Xxr6AyGptdbNAaifssNvZuMk/V7ST9z9k0vYrsvMes2sd3BwsJIeAdRBWeE3s29oKPi/dvfN2eJTZtaW1dskDYy0rbt3u3vB3QstLS216BlADZQMv5mZpPWSDrn78Olit0pakt1fImlL7dsDUC/lfKX3DkmLJb1vZvuzZSslrZH0OzNbKum4pB/Wp8XmsG3btqK1AwcOJLedMWNGsn799dcn6598kn6XNX78+KK1F198MbltR0dHso7Rq2T43f0PkqxI+fu1bQdAo3CFHxAU4QeCIvxAUIQfCIrwA0ERfiAofrq7TIVCoeJtP//886r2fe7cuWQ9dR3A3XffXdW+MXpx5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnL9Ps2bOL1tra2pLbbt++PVlfuDD926fvvfdesp4a5585c2ZyW8TFmR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmKcv0ypqa7Xr1+f3HbNmjXJ+sDAiJMdfaWzszNZr3aKb8TEmR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgio5zm9mUyS9ImmipPOSut19rZmtlrRM0mC26kp3T39xfZSaN29eVXUgD+Vc5HNO0k/d/V0z+5akd8xsV1b7hbv/a/3aA1AvJcPv7v2S+rP7n5rZIUmT690YgPq6pPf8ZtYu6XuS9mWLHjSzA2bWY2bji2zTZWa9ZtY7ODg40ioAclB2+M1snKTfS/qJu38i6ZeSpkmaqaFXBj8faTt373b3grsXWlpaatAygFooK/xm9g0NBf/X7r5Zktz9lLt/6e7nJa2TNKt+bQKotZLhNzOTtF7SIXd/btjy4T9Z+wNJB2vfHoB6KefT/jskLZb0vpntz5atlNRpZjMluaQ+ST+uS4cA6qKcT/v/IMlGKIUc0wdGC67wA4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBGXu3ridmQ1K+p9hiyZIOt2wBi5Ns/bWrH1J9FapWvb2N+5e1u/lNTT8X9u5Wa+7F3JrIKFZe2vWviR6q1RevfGyHwiK8ANB5R3+7pz3n9KsvTVrXxK9VSqX3nJ9zw8gP3mf+QHkJJfwm9lcM/tvMztqZo/m0UMxZtZnZu+b2X4z6825lx4zGzCzg8OWXWdmu8zsSHY74jRpOfW22sz+Nzt2+83sn3PqbYqZ/ZeZHTKzP5rZw9nyXI9doq9cjlvDX/ab2ZWSDku6S9IJSW9L6nT3PzW0kSLMrE9Swd1zHxM2s3+UdEbSK+5+S7bsGUkfu/ua7A/neHf/lybpbbWkM3nP3JxNKNM2fGZpSQsl/Ug5HrtEX/cqh+OWx5l/lqSj7n7M3c9K+o2kBTn00fTcfa+kjy9avEDShuz+Bg3942m4Ir01BXfvd/d3s/ufSrows3Suxy7RVy7yCP9kSX8e9viEmmvKb5e008zeMbOuvJsZwQ3ZtOkXpk9vzbmfi5WcubmRLppZummOXSUzXtdaHuEfafafZhpyuMPd/17SPEnLs5e3KE9ZMzc3yggzSzeFSme8rrU8wn9C0pRhj78t6WQOfYzI3U9mtwOSXlfzzT586sIkqdntQM79fKWZZm4eaWZpNcGxa6YZr/MI/9uSppvZd8zsm5IWSdqaQx9fY2Zjsw9iZGZjJc1R880+vFXSkuz+EklbcuzlrzTLzM3FZpZWzseu2Wa8zuUin2wo498kXSmpx92fangTIzCz72robC8NTWK6Mc/ezGyTpNka+tbXKUk/k/Qfkn4naaqk45J+6O4N/+CtSG+zNfTS9auZmy+8x25wbx2S3pT0vqTz2eKVGnp/nduxS/TVqRyOG1f4AUFxhR8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaD+D4kWARnNJFk3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at location 59999 is: 8\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# following line contains a 'magic function' to plot figures on jupyter notebooks\n",
    "%matplotlib inline \n",
    "\n",
    "digit = train_images[59999] # let's observe the last image in the training set\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "# to verify wheather the label is correct or not\n",
    "print('Value at location 59999 is: {}'.format(train_labels[59999]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's first build our convolutional neural network for this classification task. Then the above data will be fed to the model and lastly, the model will be tested on the test data to see if the model is correctly trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models and layers to define the model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Sequential neural network is used\n",
    "model = models.Sequential()\n",
    "\n",
    "# add convolution layers\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# let's add 2 dense layers to the model\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` network = models.Sequential() ``` means that the model used is Sequential. Other types of models are also there. We will look into them in other tutorials.\n",
    "\n",
    "``` model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) ``` means that this layer is of the type convolution. 32 indicates the number of filters and (3,3) indicates the size of the convolution kernal. Activation function is relu. The shape of the input to this layer is of the for 28 x 28 x 1.\n",
    "\n",
    "``` model.add(layers.MaxPooling2D((2, 2))) ``` [MaxPooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) layer is used. The use of doing maxpooling is that first of all it will make better learning of features and secondly it will help to downsample the image (intermittent features). So, when you flatten it before giveing it to the dense layer, its size will not be very large.\n",
    "\n",
    "``` network.add(layers.Dense(64, activation='relu')) ``` means that the layer that we have added is of type Dense and it contains 64 units. They are Densely connected (also known as fully connected layers). The activation function used is relu (Rectilinear unit).\n",
    "\n",
    "``` network.add(layers.Dense(10, activation='softmax')) ``` means that this layer is again of type Dense and contains 10 units. The activation function used is softmax. It is a 10 way softmax layer which means that it will return an array of 10 probabilities which will sum to 1 (each probability corresponds to each class from 0 to 9).\n",
    "\n",
    "Now, let's look at the summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the first layer's output size is 26 x 26 x 32. Why 26 x 26 and not 28 (since, input size is 28 x 28)? [This](https://en.wikipedia.org/wiki/Convolutional_neural_network) might be helpful in understanding it. As a rule of thumb, just remember that with default stride (=1), the output from any convolutional 2D layer is (input-2). 32 are the number of filters used.\n",
    "\n",
    "To calculate the number of parameters (in this case), use the following [formula](https://stackoverflow.com/a/42787467):\n",
    "#parameters = (#input_filters * filter_size + b) * #output_filters\n",
    "\n",
    "where,\n",
    "b = Number of bias term, which is equal to 1.\n",
    "filter_size = 3 * 3 (for example)\n",
    "\n",
    "Now, let's compile the model by using appropriate optimizer, loss function and evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is ready now. We'll just need to feed the data into the model. To feed in the data, the data needs to be reshaped in the form which is expected by the model. We'd also need to convert all the values to the closed inveral [0,1]. To convert each image in the interval 0 to 1, we'd need some information about these images. Since, these images are greyscale, the maximum value must be 255 and minimum must be 0. We can also check this from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1)) # since the input to the model is of the shape 28x28\n",
    "train_images = train_images.astype('float32')/255 # model expects floating point values in 99% of the cases\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)) # since the input to the model is of the shape 28x28\n",
    "test_images = test_images.astype('float32')/255 # model expects floating point values in 99% of the cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also convert labels to binary categorical form. Keras provides an inbuilt function for doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data se entirely ready to be fed into the neural network. We will use fit() function to train the model. The first parameter to the following function are the training images, next argument is the label list. The named argument epoch specifies the number of iterations. Batch size defines the number of samples you want to use at one go.\n",
    "\n",
    "Note that, 1 iteration corresponds to training of all the samples not just 128 (in this case).  Batch represents the number of samples you want to use at a time. (More about it in further tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.2383 - acc: 0.9240\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.0547 - acc: 0.9828\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0367 - acc: 0.9886\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0265 - acc: 0.9917\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0212 - acc: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1188fefd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two quantities can be seen after each iteration. Loss and the accuracy of the model. After 5 iterations, the model has achieved 98.89% accuracy. Now, let's see how is it performing on the test set. We use evaluate() function for evaluating it on the test set. The function returns the loss and the accuracy of the model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 514us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9928"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved 99.28% accuracy on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
